{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load danh sách các từ stop word\n",
    "stoplist = []\n",
    "\n",
    "with open(\"./data/vnstopword.txt\", encoding=\"utf-8\") as f :\n",
    "  text = f.read()\n",
    "  for word in text.split('\\n') :  #Tách ra mỗi dòng là 1 từ stopword riêng lẻ\n",
    "      stoplist.append(word)\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "# Load mô hình trained word2vec để embedding\n",
    "word2vec_model_path = (\"./data/w2v.bin\")\n",
    "\n",
    "#w2v = Word2Vec.load(word2vec_model_path)\n",
    "w2v = KeyedVectors.load_word2vec_format(word2vec_model_path,binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm hoán đổi ngẫu nhiên vị trí các từ trong câu. Số lượng câu mới tạo ra là n\n",
    "\n",
    "def Random_Swap(sentence,n):\n",
    "    new_sentences = []\n",
    "    words = sentence.split()\n",
    "    for i in range(n):\n",
    "        random.shuffle(words)\n",
    "        new_sentences.append(' '.join(words))\n",
    "    return new_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm xoá từ trong câu nếu từ đó là: Động từ (V), giới từ (C), số (M), dấu câu (F)\n",
    "\n",
    "def Random_Deletion(sentence):\n",
    "    \n",
    "    new_sentence = []\n",
    "    tagged_word = ViPosTagger.postagging(sentence)\n",
    "    \n",
    "    # Chia POS thành 2 list cho dễ thực hiện\n",
    "    word = tagged_word[0]\n",
    "    tag = tagged_word[1]\n",
    "\n",
    "    edited_sentence = [i for i,j in zip(word,tag) if j != 'P' and j != 'V' and j != 'C' and j != 'F' and j != 'M']\n",
    "    edited_sentence = ' '.join(edited_sentence)\n",
    "    new_sentence.append(edited_sentence)\n",
    "    return new_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm thay thế từ đồng nghĩa\n",
    "\n",
    "def syn_rep(sentence, synonyms_lexicon):\n",
    "    keys = synonyms_lexicon.keys()\n",
    "    words = sentence.split()\n",
    "    n_sentence = sentence\n",
    "    for w in words:\n",
    "        if w not in stoplist:\n",
    "            if w in keys:\n",
    "                n_sentence = n_sentence.replace(w, synonyms_lexicon[w][0])  # Thay đổi từ đồng nghĩa ở cột kế tiếp\n",
    "    return n_sentence\n",
    "\n",
    "#===================================================================================\n",
    "\n",
    "def Synonym_Replacement(sentence):\n",
    "    #Get synonyms word from this file\n",
    "    synonyms_lexicon = get_synonyms('./data/vietnamsyn.txt')\n",
    "    \n",
    "    new_sentence = []\n",
    "    \n",
    "    sen_replaced = syn_rep(sentence, synonyms_lexicon)\n",
    "    new_sentence.append(sen_replaced)\n",
    "    \n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm lấy từ đồng nghĩa trong file vietnamsyn.txt\n",
    "#synonyms_lexicon = get_synonyms('./vietnamsyn.txt')\n",
    "\n",
    "def get_synonyms(path):\n",
    "    synonyms_lexicon = {}\n",
    "    text_entries = [l.strip() for l in open(path, encoding=\"utf8\").readlines()]\n",
    "    for e in text_entries:\n",
    "        e = e.split('\\t')\n",
    "        k = e[0]\n",
    "        v = e[1:len(e)]\n",
    "        synonyms_lexicon[k] = v\n",
    "    return synonyms_lexicon\n",
    "\n",
    "#===================================================================================\n",
    "def Insert(sentence, synonyms_lexicon):\n",
    "    keys = synonyms_lexicon.keys()\n",
    "    words = sentence.split()\n",
    "    n_sentence = sentence\n",
    "    for w in words:\n",
    "        if w not in stoplist:\n",
    "            if w in keys:\n",
    "                n_sentence = n_sentence + ' ' + synonyms_lexicon[w][0] # Chèn từ đồng nghĩa vào cuối câu.\n",
    "    return n_sentence\n",
    "\n",
    "#===================================================================================\n",
    "\n",
    "def Random_Insert(sentence):\n",
    "    #Get synonyms word from this file\n",
    "    synonyms_lexicon = get_synonyms('./data/vietnamsyn.txt')\n",
    "    \n",
    "    new_sentence = []\n",
    "    \n",
    "    sen_inserted = Insert(sentence, synonyms_lexicon)\n",
    "    new_sentence.append(sen_inserted)\n",
    "    \n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out a similarity word from word embedding space\n",
    "\n",
    "def Similarity(word):\n",
    "    # Lấy ra 1 similarity word đầu tiên với score lớn nhất\n",
    "    word_similarity = w2v.most_similar(word,topn=1)\n",
    "    \n",
    "    # Vòng lặp này để trả về word đầu tiên, với similarity score là lớn nhất\n",
    "    for x in word_similarity:\n",
    "        word = ''.join(x[0]) # Lệnh này lấy ra chữ, bỏ qua score\n",
    "    return word   \n",
    "\n",
    "#===================================================================================\n",
    "# Repalcement similitary word from word vector embedding space\n",
    "\n",
    "def Word_Replacement(sentence):\n",
    "    words = sentence.split()\n",
    "    replaced_sentence = []\n",
    "    new_sentence = ''\n",
    "    for word in words:\n",
    "        if word not in w2v:\n",
    "            new_sentence = new_sentence + ' ' + word\n",
    "        else: \n",
    "            if word in stoplist:\n",
    "                new_sentence = new_sentence + ' ' + word\n",
    "            else:\n",
    "                new_word = Similarity(word)\n",
    "                new_sentence = new_sentence + ' ' + new_word\n",
    "                \n",
    "    replaced_sentence.append(new_sentence.strip())\n",
    "    return replaced_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file data to augmentate\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "training_data = load_files(r\"./data2\", encoding=\"utf-8\")\n",
    "X_train, y_train = training_data.data, training_data.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Số comment, 3 pos, 3 neg\n",
    "#print(X_train[1],y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đoạn code này là đọc tất cả các comment và label. Thực hiện agumentation.\n",
    "# Sau đó lưu vào file .csv\n",
    "import csv   \n",
    "header=['comment','label']\n",
    "with open(r'data2.csv', 'a', encoding='utf8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    for i in range(0,len(X_train)):\n",
    "        comment = str(X_train[i])\n",
    "        label = y_train[i]\n",
    "        data = [comment,label]\n",
    "        writer.writerow(data)\n",
    "        \n",
    "        # Thực hiện swapping\n",
    "        data= [Random_Swap(comment,1),y_train[i]]\n",
    "        writer.writerow(data)\n",
    "    \n",
    "        \n",
    "        # Thực hiện việc xoá ngẫu nhiên\n",
    "        data= [Random_Deletion(comment),y_train[i]]\n",
    "        writer.writerow(data)\n",
    "        \n",
    "        # Thực hiện việc chèn từ đồng nghĩa cuối câu\n",
    "        data= [Random_Insert(comment),y_train[i]]\n",
    "        writer.writerow(data)\n",
    "        \n",
    "        # Thực hiện việc thay thế từ gần nghĩa lấy từ word embedding\n",
    "        data= [Word_Replacement(comment),y_train[i]]\n",
    "        writer.writerow(data)\n",
    "        \n",
    "        # Thực hiện thay thế từ đồng nghĩa lấy từ file từ đồng nghĩa\n",
    "        #data= [Synonym_Replacement(comment),y_train[i]]\n",
    "        #writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sau khi augment xong lưu nội dung vào file .csv\n",
    "\n",
    "import pandas as pd\n",
    "a = pd.read_csv('data2.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
